% vim:ts=4:sw=4
%
% Copyright (c) 2008-2009 solvethis
% Copyright (c) 2010-2012 Casper Ti. Vector
% Public domain.

\chapter{Classification with SVMs and the RCBHT}
In this section we will introduce a binary classifier using Support Vector Machines and feature inputs from RCBHT labels. The first three layers of the RCBHT taxonomy are of relevant importance to the classification problem. Labels from these three layers will be used to construct fixed-length feature vectors. The premise of constructing a feature vector out of RCBHT labels is that success cases have similar patterns in all six axis and similar patterns indicate the similar gradient of each separation of the signal.

\indent The left four signal figures are success cases and right four are failure cases before Rotation state. The success cases are in similar patterns while the failure cases are different from the success cases diversely. With diverse gradients of different patterns, RCBHT can be used in our work.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.3]{./img/1success.eps}
    \includegraphics[scale=0.3]{./img/1failure.eps}
    \caption{Success Fx signal and Failure Fx signal}
    \label{sfsignal}
\end{figure}


\indent Thus, feature vectors were created with RCBHT. Our feature vectors are composed of gradient labels at three different levels of abstraction that characterize an assembly task. The taxonomy RCBHT is based on relative change. THe reason for choosing RCBHT is that this taxonomy is specially developed for this task. Also, the feature vector constructed with RCBHT has much fewer dimension, at most 9, than typical feature, which may use 2000 sample points for 10-second assembly with sampling frequency of 200Hz. \\
\indent This approach with SVM all based on one assumption when the classifier was trained, it could only be used for assembly with same configuration. Different geometric snap, changing assembly strategy, different sensor position etc. would change the signal patterns which would be misidentified. Any change of configuration required retraining under that configuration. \\
\indent Then comes to our approach setup. In the first subsection following, we will describe our approach to construct feature in detail. The second subsection will talk about our work with SVM briefly. 
\section{Feature Vector Constructing}
\indent With the RCBHT, we sample the whole assembly using Primitive labels, Motion Composites labels, Low-level behavior labels. Each axis, $F_{i}$, contains n entries, each represent a label type $e_{j}$. Such that $F_{i} = {e_{1}, e_{2}, \dots , e_{n}}$. The whole vector is consist of six axis, $F_{x}, F_{y}, F_{z}, M_{x}, M_{y}, M_{z}$. 9 labels independently of each axis is consisted in primitive layer, 7 of MC and 7 of LLB. The feature vector will be conducted containing 6 axis, as $[F_{x}, F_{y}, F_{z}, M_{x}, M_{y}, M_{z}]$. Take the vector representing Primitive layer if in $F_{x}$, bpos occurs twice, mpos occurs once, and the other axis do not have labels (which is an impossible case but just for better understanding), the final vector will be constructed as $[2, 1,\underbrace{0,0, \dots, 0}_\text{52 zeros}]$     \\
\indent Detailed representation for Primitive layer, Motion Composites layer, Low-level behavior layer can be seem in \ref{featureVector}.
\begin {table}[h]
\centering
\caption {Feature Vector Representation}
\label {featureVector}
\begin {tabular}{|llllllllll|}
\hline
Vector Position & 1     & 2     & 3     & 4     & 5     & 6     & 7     & 8     & 9     \\ \hline
Primitive Layer & bpos  & mpos  & spos  & bneg  & mneg  & sneg  & cons  & pimp  & nimp  \\ \hline
MC Layer        & a     & i     & d     & k     & pc    & nc    & c     &       &       \\ \hline
LLB Layer       & FX    & CT    & PS    & PL    & AL    & SH    & U     & N     &       \\ \hline
\end {tabular}
\end {table}
\section{Classify Feature Vector}
After constructing feature vectors, classifer should be trained between the success and failure cases. Among different techniques for supervised classification, linear Support Vector Machine is chosen. In this section, we will briefly talk about classifier implementation and review Support Vector Machine and kernel function.\\
\section{Support Vector Machine Review}
The Support Vector Machine methos is to find a hyperplane that separate the cases with different labels. Nice prediction are made when all cases, represented by points, are far away from the hyperplane, which means all the hyphothesis are creditable. The hyperplane can be represented as:$\omega^{T}x+b=0$. Here $\omega^{T}$ is the multiply factor of the hyperplane when b means the bias from the zero point. Each point, the deviation can be represented as: \\
\begin{equation}
    \hat{\gamma}^{(i)}=y^{(i)}(\omega^{(i)}x+b)
\end{equation}
Here $(y^{(i)}, x^{(i)}$ is a single case when $y^{(i)}$ is the label whether this case was success or not represented in {1, -1} respectively and $x^{(i)}$ is the vector input for training and testing. $\hat{\gamma}^{(i)}$ is the functional margin of this case. To have a nice hyperplane, we need to let as many as possible points to get as far away as well from the hyperplane. That's to say,
\begin{equation}
    \begin{split}
        max\quad&\gamma \nonumber \\
        s.t.\quad&\gamma = \underset{i = 1,\ldots,m}{min}\hat{\gamma}
    \end{split}
\end{equation}
Here $\gamma$ is the geometric margin of points from hyperplane. This equation shows that the nature of SVM is to find a hyperplane to maximize $\gamma$, which is the least functional margin of $\hat{\gamma}$.
\section{Classifier Implementation}
\indent In our work, libsvm \cite{CC01a} is used with Gaussian Kernel \cite{keerthi2003asymptotic}, which is recommended as the first try kernel. After labeling each feature vector with 1 or -1 to present success or failure, the cases with labels were then trained with Support Vector Machine and get a hyperplane. The two kind of cases were separated in the two sides. With this hyperplane, new cases can be predicted. \\
