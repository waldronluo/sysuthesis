% vim:ts=4:sw=4
%
% Copyright (c) 2008-2009 solvethis
% Copyright (c) 2010-2012 Casper Ti. Vector
% Public domain.

\chapter{Classification with RCBHT using SVM}
In this section we will introduce a binary classifier using Support Vector Machines and feature inputs from RCBHT labels. The first three layers of the RCBHT taxonomy are of relevant importance to the classification problem. Labels from these three layers will be used to construct fixed-length feature vectors. The premise of constructing a feature vector out of RCBHT labels is that success cases have similar patterns in all six axis and similar patterns indicate the similar gradient of each separation of the signal.


\indent Need pictures of success cases picture and pictures of failure cases.



\indent In this section our approach will be introduced. The RCBHT is used to create a feature vector. Our feature vectors are composed of gradient labels at three different levels  which is not data-driven method but gradient-driven. The taxonomy RCBHT is based on relative change. When a delay happens, usually, at the very begining of the whole process, with RCBHT, the vector will be the same as the delay didn't happen because the delay will be catagorized with neighbour label cons (In Primitive layer). Another reason for choosing RCBHT is that this taxonomy is specially developed for this task. The increasingly abstracted hierarchical taxonomy provides date-rich information. Varying layers of labels can construct to be different feature vectors and data-rich feature vectors provide more possible for this task.\\ 
\indent This two approachs with SVM all based on one assumption when the classifier was trained, it could only be used for assembly with same configuration. Different geometric snap, changing assembly strategy, different sensor position etc. would change the signal patterns which would be misidentified. Any change of configuration required retraining under that configuration. \\
\indent Then comes to our approach setup. In the first subsection following, we will describe our approach to construct feature in detail. The second subsection will talk about our work with SVM briefly. 
\section{Feature Vector Constructing}
\indent With RCBHT, we sample the whole process using Primitive labels, Motion Composites labels, Low-level behavior labels. Each axis, $F_{i}$, contains n entries, each represent a kind of label $e_{j}$. Such that $F_{i} = {e_{1}, e_{2}, \dots , e_{n}}$. The whole vector is consist of six axis, $F_{x}, F_{y}, F_{z}, M_{x}, M_{y}, M_{z}$, and the vector will be conducted as $[F_{x}, F_{y}, F_{z}, M_{x}, M_{y}, M_{z}]$. Take the vector representing Primitive layer for example. If in $F_{x}$, bpos occurs twice, mpos occurs once, and the other axis do not have labels (which is an impossible case but just for better understanding), the final vector will be constructed as $[2, 1,\underbrace{0,0, \dots, 0}_\text{52 zeros}]$     \\
\indent Detailed representation for Primitive layer, Motion Composites layer, Low-level behavior layer can be seem in \ref{featureVector}.
\begin {table}[h]
\centering
\caption {Feature Vector Representation}
\label {featureVector}
\begin {tabular}{|llllllllll|}
\hline
Vector Position & 1     & 2     & 3     & 4     & 5     & 6     & 7     & 8     & 9     \\ \hline
Primitive Layer & bpos  & mpos  & spos  & bneg  & mneg  & sneg  & cons  & pimp  & nimp  \\ \hline
MC Layer        & a     & i     & d     & k     & pc    & nc    & c     &       &       \\ \hline
LLB Layer       & FX    & CT    & PS    & PL    & AL    & SH    & U     & N     &       \\ \hline
\end {tabular}
\end {table}
\section{Classify Feature Vector}
After constructing feature vectors, classifer should be trained between the success and failure cases. Among different techniques for supervised classification, linear Support Vector Machine is chosen. In this section, we will briefly talk about classifier implementation and review Support Vector Machine and kernel function.\\
\section{Support Vector Machine Review}
The Support Vector Machine methos is to find a hyperplane that separate the cases with different labels. Nice prediction are made when all cases, represented by points, are far away from the hyperplane, which means all the hyphothesis are creditable. The hyperplane can be represented as:$\omega^{T}x+b=0$. Here $\omega^{T}$ is the multiply factor of the hyperplane when b means the bias from the zero point. Each point, the deviation can be represented as: \\
\begin{equation}
    \hat{\gamma}^{(i)}=y^{(i)}(\omega^{(i)}x+b)
\end{equation}
Here $(y^{(i)}, x^{(i)}$ is a single case when $y^{(i)}$ is the label whether this case was success or not represented in {1, -1} respectively and $x^{(i)}$ is the vector input for training and testing. $\hat{\gamma}^{(i)}$ is the functional margin of this case. To have a nice hyperplane, we need to let as many as possible points to get as far away as well from the hyperplane. That's to say,
\begin{equation}
    \begin{split}
        max\quad&\gamma \nonumber \\
        s.t.\quad&\gamma = \underset{i = 1,\ldots,m}{min}\hat{\gamma}
    \end{split}
\end{equation}
Here $\gamma$ is the geometric margin of points from hyperplane. This equation shows that the nature of SVM is to find a hyperplane to maximize $\gamma$, which is the least functional margin of $\hat{\gamma}$.
\section{Implementation of classifer}
\indent In our work, libsvm \cite{CC01a} is used with Gaussian Kernel \cite{keerthi2003asymptotic}, which is recommended as the first try kernel. After labeling each feature vector with 1 or -1 to present success or failure, the cases with labels were then trained with Support Vector Machine and get a hyperplane. The two kind of cases were separated in the two sides. With this hyperplane, new cases can be predicted. \\
